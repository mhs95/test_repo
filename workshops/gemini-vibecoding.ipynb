{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOifJ8x3FgbE"
      },
      "source": [
        "## Gemini BuildWithAI 2025 Workshop\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/mashhoodr/gemini-cookbook/blob/main/workshops/gemini-vibecoding.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "\n",
        "\n",
        "This notebook is designed to help you become familiar with Gemini's API while creating some simple projects. We use \"vibe coding\" to help build some basic apps and test out the power of Gemini.\n",
        "\n",
        "### Learning Outcomes\n",
        "\n",
        "The objective of this workshop is to help the attendees become familiar with the offerings of Google Gemini, and give them an opportunity to try out the API themselves. We run through a few exercises to help understand the use cases for the different functionalities present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78fNdcGFgbG"
      },
      "source": [
        "### Authentication\n",
        "\n",
        "The Gemini API uses API keys for authentication. We will now setup the API key in this colab - and test out our authentication. Your trainer has already demoed the instructions below.\n",
        "\n",
        "You can [create](https://aistudio.google.com/app/apikey) your API key using Google AI Studio with a single click.  \n",
        "\n",
        "Remember to treat your API key like a password. Do not accidentally save it in a notebook or source file you later commit to GitHub. This notebook shows you two ways you can securely store your API key.\n",
        "\n",
        "* If you are using Google Colab, we recommend you store your key in Colab Secrets.\n",
        "\n",
        "* If you are using a different development environment (or calling the Gemini API through `cURL` in your terminal), we recommend you store your key in an environment variable.\n",
        "\n",
        "Let's start with Colab Secrets.\n",
        "\n",
        "Add your API key to the Colab Secrets manager to securely store it.\n",
        "\n",
        "1. Open your Google Colab notebook and click on the ðŸ”‘ **Secrets** tab in the left panel.\n",
        "   \n",
        "   <img src=\"https://storage.googleapis.com/generativeai-downloads/images/secrets.jpg\" alt=\"The Secrets tab is found on the left panel.\" width=50%>\n",
        "\n",
        "2. Create a new secret with the name `GOOGLE_API_KEY`.\n",
        "3. Copy/paste your API key into the `Value` input box of `GOOGLE_API_KEY`.\n",
        "4. Toggle the button on the left to allow notebook access to the secret.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9Yddv_hFgbH"
      },
      "source": [
        "### Install the Python SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU2jQW1UFgbI",
        "outputId": "e62fb250-d52c-4a69-a675-0868897a88e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/196.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.3/196.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf5afjtmFgbJ"
      },
      "source": [
        "### Configure the SDK with your API key.\n",
        "\n",
        "You'll call `genai.configure` with your API key, but instead of pasting your key into the notebook, you'll read it from Colab Secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Le2dJEqVFgbJ"
      },
      "outputs": [],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "from IPython.display import Markdown, HTML, Image\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFBQrPA_FgbK"
      },
      "source": [
        "And that's it! Now you're ready to use the Gemini API.\n",
        "\n",
        "Now lets set our model we want to use throughout this notebook. You can change this to any available model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uzb-ln5iFgbK"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL = \"gemini-2.5-flash-preview-05-20\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq-oYLxXFgbK"
      },
      "source": [
        "### Running your first prompt\n",
        "\n",
        "Use the `generate_content` method to generate responses to your prompts. You can pass text directly to generate_content, and use the `.text` property to get the text content of the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uVSIm0GLFgbK",
        "outputId": "528a3ffa-0e14-47f0-8374-ebbb2e613adf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI can enhance cricket in several key ways:\n",
            "\n",
            "1.  **Personalized Player Training & Performance Analysis:** AI can analyze biomechanics, identify flaws in batting or bowling actions, predict injury risks, and suggest tailored training drills to optimize individual performance.\n",
            "2.  **Real-time Strategy & Opposition Analysis:** AI algorithms can process vast amounts of historical and real-time data to suggest optimal field placements, bowling changes, batting matchups, and predict likely outcomes, giving teams a strategic edge.\n",
            "3.  **Enhanced Officiating & Decision Making:** AI vision systems can assist umpires with accurate no-ball detection, precise run-out calls, and improved ball-tracking for DRS, leading to fairer and more consistent decisions.\n",
            "4.  **Talent Identification & Scouting:** AI can sift through player data from various leagues and age groups to identify promising talent, predict future potential, and match players to specific team needs.\n",
            "5.  **Fan Engagement & Broadcasting:** AI can generate real-time statistics, predictive match outcomes (e.g., win probability), personalized highlight reels, and enhance broadcast graphics with augmented reality, offering a richer viewing experience.\n"
          ]
        }
      ],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL, contents= \"How can AI enhance cricket? Explain briefly\" #\"Explain how AI works in a few words.\"\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHT9ZgKzFgbL"
      },
      "source": [
        "> *Do it yourself: Update the above using a different Gemini version available. Did the response change?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GAm0htLFgbL"
      },
      "source": [
        "#### Use images in your prompt\n",
        "\n",
        "Here we download an image from a URL and pass that image in our prompt.\n",
        "\n",
        "First, we download the image and load it with PIL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s_L2VZkFgbL"
      },
      "outputs": [],
      "source": [
        "!curl -o image.jpg https://storage.googleapis.com/generativeai-downloads/images/jetpack.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8w_rp6-FgbL"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3SwjwDdFgbL"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"This image contains a sketch of a potential product along with some notes.\n",
        "Given the product sketch, describe the product as thoroughly as possible based on what you\n",
        "see in the image, making sure to note all of the product features. Return output in json format:\n",
        "{description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgsqMMxfFgbL"
      },
      "source": [
        "Then we can include the image in our prompt by just passing a list of items to `generate_content`. You can pass in multiple images, or prompts or files as per your requirement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Th_yvCy7FgbL"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[prompt, img]\n",
        ")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfyDa5l2FgbM"
      },
      "source": [
        "#### Uploading files\n",
        "\n",
        "For types other than image, like audio, video or pdf - you can use the `upload_file` function to send data to Gemini.\n",
        "\n",
        "The following list of documents are supported:\n",
        "\n",
        "- PDF - application/pdf\n",
        "- JavaScript - application/x-javascript, text/javascript\n",
        "- Python - application/x-python, text/x-python\n",
        "- TXT - text/plain\n",
        "- HTML - text/html\n",
        "- CSS - text/css\n",
        "- Markdown - text/md\n",
        "- CSV - text/csv\n",
        "- XML - text/xml\n",
        "- RTF - text/rtf\n",
        "\n",
        "First download a PDF file into Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8dyLPyOFgbM"
      },
      "outputs": [],
      "source": [
        "URL = \"https://storage.googleapis.com/generativeai-downloads/data/Smoothly%20editing%20material%20properties%20of%20objects%20with%20text-to-image%20models%20and%20synthetic%20data.pdf\"\n",
        "!wget -q $URL -O sample.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAxQqIq6FgbM"
      },
      "source": [
        "Then pass it into Gemini."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6er8Dt7FgbM"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import io\n",
        "import httpx\n",
        "\n",
        "\n",
        "your_file = client.files.upload(file='sample.pdf')\n",
        "prompt = \"Can you summarize this file as a bulleted list?\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "  model=MODEL,\n",
        "  contents=[your_file, prompt])\n",
        "\n",
        "Markdown(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6NQnxl1FgbM"
      },
      "source": [
        "#### Supply a schema through model configuration\n",
        "The following example does the following:\n",
        "\n",
        "Instantiates a model configured through a schema to respond with JSON.\n",
        "Prompts the model to return cookie recipes.\n",
        "\n",
        "The Gemini API Python client library supports schemas defined with the following types (where AllowedType is any allowed type):\n",
        "\n",
        "- string -> enum, format, nullable\n",
        "- integer -> format, minimum, maximum, enum, nullable\n",
        "- number -> format, minimum, maximum, enum, nullable\n",
        "- boolean -> nullable\n",
        "- array -> minItems, maxItems, items, nullable\n",
        "- object -> properties, required, propertyOrdering, nullable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dguk4fKRFgbM"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Recipe(BaseModel):\n",
        "  recipe_name: str\n",
        "  ingredients: list[str]\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='List a few popular cookie recipes.',\n",
        "    config={\n",
        "        'response_mime_type': 'application/json',\n",
        "        'response_schema': list[Recipe],\n",
        "    },\n",
        ")\n",
        "\n",
        "# Use the response as a JSON string.\n",
        "print(response.text)\n",
        "\n",
        "# Use instantiated objects.\n",
        "my_recipes: list[Recipe] = response.parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiNn3yD3FgbM"
      },
      "source": [
        "#### Use an enum to constrain output\n",
        "In some cases you might want the model to choose a single option from a list of options. To implement this behavior, you can pass an enum in your schema. You can use an enum option anywhere you could use a str in the response_schema, because an enum is a list of strings. Like a JSON schema, an enum lets you constrain model output to meet the requirements of your application.\n",
        "\n",
        "For example, assume that you're developing an application to classify musical instruments into one of five categories: \"Percussion\", \"String\", \"Woodwind\", \"Brass\", or \"\"Keyboard\"\". You could create an enum to help with this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qSH528ZFgbM"
      },
      "outputs": [],
      "source": [
        "import enum\n",
        "\n",
        "class Instrument(enum.Enum):\n",
        "  PERCUSSION = \"Percussion\"\n",
        "  STRING = \"String\"\n",
        "  WOODWIND = \"Woodwind\"\n",
        "  BRASS = \"Brass\"\n",
        "  KEYBOARD = \"Keyboard\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents='What type of instrument is an oboe?',\n",
        "    config={\n",
        "        'response_mime_type': 'text/x.enum',\n",
        "        'response_schema': Instrument,\n",
        "    },\n",
        ")\n",
        "\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx-tAWzhFgbM"
      },
      "source": [
        "#### Have a chat\n",
        "\n",
        "The Gemini API enables you to have freeform conversations across multiple turns.\n",
        "\n",
        "The [ChatSession](https://ai.google.dev/api/python/google/generativeai/ChatSession) class will store the conversation history for multi-turn interactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0h-B3iRFgbM"
      },
      "outputs": [],
      "source": [
        "chat = client.chats.create(model=MODEL)\n",
        "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WnHvHJrFgbN"
      },
      "source": [
        "You can see the chat history:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl7v_jdWFgbN"
      },
      "outputs": [],
      "source": [
        "for message in chat.get_history():\n",
        "    print(f'role - {message.role}',end=\": \")\n",
        "    print(message.parts[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF-at6yoFgbN"
      },
      "source": [
        "You can send another message to continue the conversation. The previous conversation is automatically sent in the next message as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfQiSX2XFgbN"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"What are the main components of a computer?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0rjKufQFgbN"
      },
      "source": [
        "### Setting the system instruction\n",
        "\n",
        "The system instruction in Gemini is a tool for developers to fine-tune the model's responses for specific tasks. It lets them define various aspects of how Gemini should generate responses [2].\n",
        "\n",
        "Here are some key benefits of system instructions:\n",
        "\n",
        "**Role definition:** You can specify the role Gemini should play, such as a home-cooking assistant or a music historian.\n",
        "\n",
        "**Format control:** Instruct Gemini on the format of the response, like text, a list, or even a structured JSON object.\n",
        "\n",
        "**Goal setting:** Clearly define the goal you want Gemini to achieve, making the response more focused and relevant.\n",
        "\n",
        "**Rule establishment:** Set rules for Gemini to follow, ensuring the response adheres to your specific requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4yw0HmIFgbN"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "    model=MODEL,\n",
        "    contents=[\"Share a short story for children on kindness.\"],\n",
        "    config=types.GenerateContentConfig(\n",
        "        max_output_tokens=500,\n",
        "        temperature=0.1,\n",
        "        system_instruction=\"You are a primary school teacher specializing in early childhood education. Use positive reinforcement and interactive methods to teach basic concepts. Adapt your responses to the learning style of a young child.\"\n",
        "    )\n",
        ")\n",
        "print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPOPLHYtFgbN"
      },
      "source": [
        "## Let's do some practice!\n",
        "\n",
        "Follow the instructions for the excercise below and add your code in the section after. You can use AI Studio or Gemini app to generate most of the code for the exercises below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZH_8WqcFgbN"
      },
      "source": [
        "### Exercise 1: A Simple Story Generator with Gemini Chat API\n",
        "\n",
        "**Problem statement**\n",
        "\n",
        "When kids are learning how to read - for every kid its a very difficult and slow process. The only way to get better is practice. However the practice needs to be at the right level, simply reading the same book a thousand times isnt good enough.\n",
        "\n",
        "Enter our story generator.\n",
        "1. It can be designed to generate content at the right level, so for example grade one.\n",
        "2. It can generate new content every time\n",
        "3. It can generate content which appeals to the child, where they are the hero (check link below).\n",
        "\n",
        "The right product will help the kids learn to read better over time.\n",
        "\n",
        "**Theme Selection**\n",
        "\n",
        "Present a list of themes (e.g., fantasy, sci-fi, mystery, historical) when the session starts.\n",
        "Allow the user to input a theme or select from the list.\n",
        "\n",
        "**Initial Story Generation**\n",
        "\n",
        "Based on the selected theme, generate a short paragraph introducing the story and the user's character. Ensure its safe for children, uses easy to use language and as creative as possible.\n",
        "\n",
        "**Action Selection**\n",
        "\n",
        "Provide multiple action choices related to the current story.\n",
        "Allow the user to select an action.\n",
        "\n",
        "**Story Continuation**\n",
        "\n",
        "Generate a new paragraph based on the user's chosen action, advancing the story.\n",
        "Repeat steps 3 and 4 until a desired story length or ending condition is reached.\n",
        "\n",
        "Idea is inspired by https://www.wander.ly/ - check their website for more inspiration.\n",
        "\n",
        "**Execution Plan**\n",
        "\n",
        "Think about generating a short story first, what elements from above do you need to configure.\n",
        "Use the schemas from above.\n",
        "Think about the constraints - what should it generate, and what shouldnt it generate.\n",
        "Use the input function to take relevant inputs from the user.\n",
        "Configure it to generate some actions for the next generation.\n",
        "Present those actions for selection\n",
        "Generate the next part until the user says stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DY1cv78EFgbN"
      },
      "outputs": [],
      "source": [
        "# Exercise 1 solution here..\n",
        "\n",
        "def choose_theme():\n",
        "    \"\"\"Presents a list of themes and allows the user to choose one.\"\"\"\n",
        "    themes = [\"Fantasy\", \"Sci-Fi\", \"Mystery\", \"Historical\", \"Adventure\", \"Animal\"]\n",
        "    print(\"Choose a theme for your story:\")\n",
        "    for i, theme in enumerate(themes):\n",
        "        print(f\"{i+1}. {theme}\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            choice = int(input(\"Enter the number of your chosen theme: \"))\n",
        "            if 1 <= choice <= len(themes):\n",
        "                return themes[choice - 1]\n",
        "            else:\n",
        "                print(\"Invalid choice. Please enter a number between 1 and\", len(themes))\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Please enter a number.\")\n",
        "\n",
        "\n",
        "print(\"Welcome to the Story Generator!\")\n",
        "user_name = input(\"What is your name? (This will be the name of the hero) \") or \"You\" # Allows empty names\n",
        "theme = choose_theme()\n",
        "\n",
        "# continue solution here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK5jm03sFgbN"
      },
      "source": [
        "#### Let's continue to dive deeper with Function calling\n",
        "\n",
        "To use function calling, pass a list of functions to the `tools` parameter when creating a [`GenerativeModel`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel). The model uses the function name, docstring, parameters, and parameter type annotations to decide if it needs the function to best answer a prompt.\n",
        "\n",
        "> Important: The SDK converts function parameter type annotations to a format the API understands (`glm.FunctionDeclaration`). The API only supports a limited selection of parameter types, and the Python SDK's automatic conversion only supports a subset of that: `AllowedTypes = int | float | bool | str | list['AllowedTypes'] | dict`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wrU8_-2FgbO"
      },
      "outputs": [],
      "source": [
        "def add(a:float, b:float):\n",
        "    \"\"\"returns a + b.\"\"\"\n",
        "    return a+b\n",
        "\n",
        "def subtract(a:float, b:float):\n",
        "    \"\"\"returns a - b.\"\"\"\n",
        "    return a-b\n",
        "\n",
        "def multiply(a:float, b:float):\n",
        "    \"\"\"returns a * b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "def divide(a:float, b:float):\n",
        "    \"\"\"returns a / b.\"\"\"\n",
        "    return a*b\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"tools\": [add, subtract, multiply, divide],\n",
        "}\n",
        "chat = client.chats.create(model=MODEL, config=config)\n",
        "response = chat.send_message('I have 57 cats, each owns 44 mittens, how many mittens is that in total?')\n",
        "response.text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkxMk1vRFgbO"
      },
      "source": [
        "However, by examining the chat history, you can see the flow of the conversation and how function calls are integrated within it.\n",
        "\n",
        "The `ChatSession.history` property stores a chronological record of the conversation between the user and the Gemini model. Each turn in the conversation is represented by a [`glm.Content`](https://ai.google.dev/api/python/google/ai/generativelanguage/Content) object, which contains the following information:\n",
        "\n",
        "*   **Role**: Identifies whether the content originated from the \"user\" or the \"model\".\n",
        "*   **Parts**: A list of [`glm.Part`](https://ai.google.dev/api/python/google/ai/generativelanguage/Part) objects that represent individual components of the message. With a text-only model, these parts can be:\n",
        "    *   **Text**: Plain text messages.\n",
        "    *   **Function Call** ([`glm.FunctionCall`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionCall)): A request from the model to execute a specific function with provided arguments.\n",
        "    *   **Function Response** ([`glm.FunctionResponse`](https://ai.google.dev/api/python/google/ai/generativelanguage/FunctionResponse)): The result returned by the user after executing the requested function.\n",
        "\n",
        " In the previous example with the mittens calculation, the history shows the following sequence:\n",
        "\n",
        "1.  **User**: Asks the question about the total number of mittens.\n",
        "1.  **Model**: Determines that the multiply function is helpful and sends a FunctionCall request to the user.\n",
        "1.  **User**: The `ChatSession` automatically executes the function (due to `enable_automatic_function_calling` being set) and sends back a `FunctionResponse` with the calculated result.\n",
        "1.  **Model**: Uses the function's output to formulate the final answer and presents it as a text response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9mN1MRwFgbS"
      },
      "outputs": [],
      "source": [
        "for content in chat.get_history():\n",
        "    print(content.role, \"->\", [(str(part.function_call) + ' -> ' + str(part.function_response)) for part in content.parts])\n",
        "    print('-'*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RD9onCaFgbS"
      },
      "source": [
        "#### Use Model Context Protocol (MCP)\n",
        "\n",
        "Model Context Protocol (MCP) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n",
        "\n",
        "MCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. You can learn more about MCP and how to use it in the documentation:\n",
        "\n",
        "https://ai.google.dev/gemini-api/docs/function-calling?example=weather#model_context_protocol_mcp\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGuKhCWOFgbS"
      },
      "source": [
        "### Exercise 2: A Weather App using Function Calling.\n",
        "\n",
        "Create a simple weather app using Python which will use function calling to fetch the latest weather when asked about for a specific location.\n",
        "\n",
        "An example prompt for it can be: `Whats the weather like in Islamabad today?`\n",
        "\n",
        "You can use the free api available on: https://www.weatherapi.com/\n",
        "\n",
        "Bonus: Configure the temperature setting as well (C or F), with C being default.\n",
        "\n",
        "When you ask for `Hows the weather in Karachi this week?` is it able to adjust the output accordingly?\n",
        "\n",
        "What happens when you ask it a question like `Do I need an umbrella in Islamabad today?`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4s5j5roiFgbS"
      },
      "outputs": [],
      "source": [
        "# Exercise 2 solution here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyrMbp-RFgbS"
      },
      "source": [
        "### Using Code Execution\n",
        "\n",
        "The Gemini API code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n",
        "\n",
        "The code execution environment includes the following libraries: altair, chess, cv2, matplotlib, mpmath, numpy, pandas, pdfminer, reportlab, seaborn, sklearn, statsmodels, striprtf, sympy, and tabulate. You can't install your own libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxzfGqqnFgbS"
      },
      "outputs": [],
      "source": [
        "response = client.models.generate_content(\n",
        "  model=MODEL,\n",
        "  contents='What is the sum of the first 50 prime numbers? '\n",
        "           'Generate and run code for the calculation, and make sure you get all 50.',\n",
        "  config=types.GenerateContentConfig(\n",
        "    tools=[types.Tool(\n",
        "      code_execution=types.ToolCodeExecution\n",
        "    )]\n",
        "  )\n",
        ")\n",
        "\n",
        "def display_code_execution_result(response):\n",
        "  for part in response.candidates[0].content.parts:\n",
        "    if part.text is not None:\n",
        "      display(Markdown(part.text))\n",
        "    if part.executable_code is not None:\n",
        "      code_html = f'<pre style=\"background-color: #BBBBEE;\">{part.executable_code.code}</pre>' # Change code color\n",
        "      display(HTML(code_html))\n",
        "    if part.code_execution_result is not None:\n",
        "      display(Markdown(part.code_execution_result.output))\n",
        "    if part.inline_data is not None:\n",
        "      display(Image(data=part.inline_data.data, format=\"png\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "display_code_execution_result(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wogA8kXxFgbS"
      },
      "source": [
        "#### Prompt Caching\n",
        "\n",
        "One of the cool new features which has been added by Gemini is prompt caching. If a part of your prompt or instructions is not changing, you can save some tokens by caching that part of the prompt. This is very useful in production where the same prompt might be used for thousands and millions of times and allows us to optimize cost.\n",
        "\n",
        "In order to use the cache, we create a cache context and then call the generate content endpoint with the additional context.\n",
        "\n",
        "First we download some data which is reused again and again across context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jbxXTGeFgbS"
      },
      "outputs": [],
      "source": [
        "!wget -q https://storage.googleapis.com/generativeai-downloads/data/a11.txt\n",
        "!head a11.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6D8H31qFgbT"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# upload the files.\n",
        "document = client.files.upload(file=\"a11.txt\")\n",
        "\n",
        "cache_model='models/gemini-2.0-flash-001'\n",
        "\n",
        "# Create a cache with a 5 minute TTL\n",
        "cache = client.caches.create(\n",
        "    model=cache_model,\n",
        "    config=types.CreateCachedContentConfig(\n",
        "      display_name='transcript',\n",
        "      system_instruction=(\n",
        "          \"You are an expert at analyzing transcripts.\"\n",
        "      ),\n",
        "      contents=[document],\n",
        "      ttl=\"300s\",\n",
        "  )\n",
        ")\n",
        "\n",
        "# Construct a GenerativeModel which uses the created cache.\n",
        "response = client.models.generate_content(\n",
        "  model = cache_model,\n",
        "  contents= (\"Find a lighthearted moment from this transcript\"),\n",
        "  config=types.GenerateContentConfig(cached_content=cache.name)\n",
        ")\n",
        "\n",
        "\n",
        "print(response.text)\n",
        "\n",
        "\n",
        "print(response.usage_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nhSRt1SFgbT"
      },
      "outputs": [],
      "source": [
        "# Once you have used the cache you can also delete it (or it expires automatically at the given time)\n",
        "client.caches.delete(name=cache.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjvO55kzFgbT"
      },
      "source": [
        "#### URL Context\n",
        "\n",
        "This is the latest functionality just dropped into the SDK, which allows you to provide Gemini with URLs as additional context for your prompt. The model can then retrieve content from the URLs and use that content to inform and shape its response.\n",
        "\n",
        "This tool is useful for tasks like the following:\n",
        "\n",
        "- Extracting key data points or talking points from articles\n",
        "- Comparing information across multiple links\n",
        "- Synthesizing data from several sources\n",
        "- Answering questions based on the content of a specific page or pages\n",
        "- Analyzing content for specific purposes (like writing a job description or creating test questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tmywmvbFgbT"
      },
      "outputs": [],
      "source": [
        "model_id = \"gemini-2.5-flash-preview-05-20\"\n",
        "\n",
        "url_context_tool = types.Tool(\n",
        "    url_context = types.UrlContext\n",
        ")\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=model_id,\n",
        "    contents=\"Compare recipes from https://www.indianhealthyrecipes.com/chicken-biryani-recipe/ and https://www.teaforturmeric.com/chicken-biryani/\",\n",
        "    config=types.GenerateContentConfig(\n",
        "        tools=[url_context_tool],\n",
        "        response_modalities=[\"TEXT\"],\n",
        "    )\n",
        ")\n",
        "\n",
        "for each in response.candidates[0].content.parts:\n",
        "    print(each.text)\n",
        "# get URLs retrieved for context\n",
        "print(response.candidates[0].url_context_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq_oZAwwFgbT"
      },
      "source": [
        "### Exercise 3: An Assessment Generator for School Teachers.\n",
        "\n",
        "This is a slightly more complex one, we definitely dont want to single shot this app. I have listed down some prompts/requirements below - you can use these to build up the app step by step. You can choose to change these prompts or add any other functionality you like as well. We want to be able to \"teach at right level\" and think about creating a mixture of seen and unseen questions. We can use the PDF link to understand the scope and extract some relevant questions from there.\n",
        "\n",
        "As you build up the functionality, think about testing this app, and how you will ensure that nothing is breaking after each iteration.\n",
        "\n",
        "1. Download the Grade 4 Maths book from here: `https://drive.google.com/file/d/1yc-jZ3r7etKX0JpMW2rSO0uOLlzZ8Ckk/view?usp=sharing`\n",
        "2. Extract one topic from Unit 1, e.g. Comparing and Ordering numbers. Pages 13 to 18.\n",
        "3. Upload this slice as a cache on Gemini, with expire of one hour.\n",
        "4. Use URL Context to add additional context for generating the assessment: `https://teaching.betterlesson.com/lesson/496389/comparing-and-ordering-whole-numbers`\n",
        "5. Use the Code Execution to solve the generated questions and build an answer sheet.\n",
        "6. Generate a formative assessment with 10 questions, including the ruberic and the answer sheet with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJKXyYd2FgbT"
      },
      "outputs": [],
      "source": [
        "# Exercise 3 code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek--KGRLFgbV"
      },
      "source": [
        "## Thank you.\n",
        "\n",
        "Thank you for attending this workshop. You can find more details about me on https://karachiwala.dev. I am available over most platforms as @mashhoodr.\n",
        "\n",
        "You can find many more examples for Gemini on the following repositories.\n",
        "\n",
        "- https://github.com/google-gemini/cookbook\n",
        "- https://github.com/GoogleCloudPlatform/generative-ai\n",
        "\n",
        "If you have any feedback on this workshop please share it with me using the following link:\n",
        "\n",
        "https://docs.google.com/forms/d/1iAEO1JSlh6GTLC0uudUxAiTDiNN_iMzfdCwDLZ_78sg/edit\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}